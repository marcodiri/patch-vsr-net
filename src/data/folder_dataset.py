import os
import random

import torch
import torchvision.transforms.v2 as v2
from torch.nn.functional import interpolate
from torch.utils.data import Dataset

from utils.data_utils import (
    get_pics_in_subfolder,
    load_img,
    parse_frame_title,
    transform,
)


class FolderDataset(Dataset):
    def __init__(
        self,
        hr_path,
        lr_path="",
        extension="jpg",
        *,
        patch_size,
        tempo_extent=10,
        hr_path_filter="",
        lr_path_filter="",
        dataset_upscale_factor=4,
        **kwargs,
    ):
        """
        Custom dataset for the training phase. The getitem method will return a couple (x, y), where x is the
        LowQuality input and y is the relative groundtruth. The relationship between the LQ and HQ samples depends on
        how the dataset is built.

        Args
            hr_path (str):
                base path of the hq dataset dir.
            lr_path (str):
                base path of the lq dataset dir. If empty the LR samples will be generated by downscaling
                the groundtruth, thus the model will be NOT trained for performing the Artifact Reduction function.
            extension (str, default="jpg"):
                extension of images.
            patch_size (int):
                width/height of the training patch. the model is going to be trained on patches,
                randomly extracted from the datasets.
            tempo_extent (int, default=10):
                frames window of a single dataset element.
            hr_path_filter (str):
                additional string to filter subfolders in the hr_path.
            lr_path_filter (str):
                additional string to filter subfolders in the lr_path.
            dataset_upscale_factor (int, default=2):
                resolution relationship between LQ and HQ samples. Must be known in advance: in our experiments,
                we encoded the clips and halved their resolution, thus in our case upscale_factor is 2.
        """

        self.hr_path = hr_path
        self.lr_path = lr_path
        self.extension = extension
        self.patch_size = patch_size
        self.tempo_extent = tempo_extent
        self.hr_path_filter = hr_path_filter
        self.lr_path_filter = lr_path_filter
        self.has_lowres = lr_path != ""
        self.upscale_factor = dataset_upscale_factor

        self.hr_keys = sorted(
            filter(
                lambda p: str(hr_path_filter) in str(p),
                get_pics_in_subfolder(hr_path, ext=extension),
            )
        )
        if self.has_lowres:
            self.lr_keys = sorted(
                filter(
                    lambda p: str(lr_path_filter) in str(p),
                    get_pics_in_subfolder(lr_path, ext=extension),
                )
            )
            assert len(self.hr_keys) == len(
                self.lr_keys
            ), "has_lowres is True but num of lr images does not correspond to hr images"

            self.hr_lr_keys = list(zip(self.hr_keys, self.lr_keys))

        self.size = len(self.hr_keys)

    def __len__(self):
        return self.size

    def __getitem__(self, item):
        if self.tempo_extent is None:
            hr_img = transform(load_img(self.hr_keys[item]))

            if self.has_lowres:
                lr_img = transform(load_img(self.lr_keys[item]))
            else:
                lr_img = interpolate(
                    hr_img,
                    scale_factor=1 / self.upscale_factor,
                    mode="bicubic",
                )

            hr_pat, lr_pat = self.crop(hr_img, lr_img)
            hr_pat, lr_pat = self.augment(hr_pat, lr_pat)

            return hr_pat, lr_pat
        else:
            hr_key = self.hr_keys[item]

            tot_frm = len(os.listdir(hr_key.parent))
            seq_hr, (hr_w, hr_h), cur_frm = parse_frame_title(hr_key.name)

            hr_frms, lr_frms = [], []

            # read frames
            for i in range(cur_frm, cur_frm + self.tempo_extent):
                if i < tot_frm:
                    frm_n = i
                else:
                    # reflect temporal paddding, e.g., (0,1,2) -> (0,1,2,1,0)
                    frm_n = 2 * tot_frm - i
                frm_key = "{}_{:03d}.{}".format(seq_hr, frm_n, self.extension)
                hr_frms.append(
                    transform(load_img(f"{self.hr_path}/{seq_hr}/{frm_key}"))
                )
                if self.has_lowres:
                    lr_key = self.hr_lr_keys[item][1]
                    seq_lr, (_, _), _ = parse_frame_title(lr_key.name)
                    frm_key = "{}_{:03d}.{}".format(seq_lr, frm_n, self.extension)
                    lr_frms.append(
                        transform(load_img(f"{self.lr_path}/{seq_lr}/{frm_key}"))
                    )

            hr_frms = torch.stack(hr_frms)  # t c h w
            if self.has_lowres:
                lr_frms = torch.stack(lr_frms)
            else:
                lr_frms = interpolate(
                    hr_frms,
                    scale_factor=1 / self.upscale_factor,
                    mode="bicubic",
                )

            hr_pats, lr_pats = self.crop(hr_frms, lr_frms)
            hr_pats, lr_pats = self.augment(hr_pats, lr_pats)

            return hr_pats, lr_pats

    def crop(self, gt_frms, lr_frms):
        gt_csz = self.patch_size * self.upscale_factor
        lr_csz = self.patch_size

        lr_h, lr_w = lr_frms.shape[-2:]
        assert (lr_csz <= lr_h) and (
            lr_csz <= lr_w
        ), "the crop size is larger than the image size"

        # crop lr
        lr_top = random.randint(0, lr_h - lr_csz)
        lr_left = random.randint(0, lr_w - lr_csz)
        lr_pats = lr_frms[..., lr_top : lr_top + lr_csz, lr_left : lr_left + lr_csz]

        # crop gt
        gt_top = lr_top * self.upscale_factor
        gt_left = lr_left * self.upscale_factor
        gt_pats = gt_frms[..., gt_top : gt_top + gt_csz, gt_left : gt_left + gt_csz]

        return gt_pats, lr_pats

    def augment(self, gt_pats, lr_pats):
        # flip
        axis = random.randint(1, 3)
        if axis == 2:
            gt_pats = v2.functional.vflip(gt_pats)
            lr_pats = v2.functional.vflip(lr_pats)
        if axis == 3:
            gt_pats = v2.functional.hflip(gt_pats)
            lr_pats = v2.functional.hflip(lr_pats)

        # rotate
        angle = (0.0, 90.0, 180.0, 270.0)[random.randint(0, 3)]
        gt_pats = v2.functional.rotate(gt_pats, angle)
        lr_pats = v2.functional.rotate(lr_pats, angle)

        return gt_pats, lr_pats
